\section{Least-Squares Fitting of Parametrized Surfaces}
\label{sec:LSQfitting}
In order to make a surface adhere as closely as possible to our desired form, some fitting is usually required. This typically involves varying some parameters in order to minimize some error. As minimizing the error squared of a function -- for example distance between a calculated point and its desired location -- is an important building block in many practical applications, extensive literature can be found regarding this. The treatment is especially well described for when the function depends linearly on its input (see for example \cite{becker2011advanced}). This is called \emph{linear least-squares fitting}. In this section, some selected subtopics relevant to the fitting of parametric surfaces are discussed.

%\begin{itemize}
%\item Possibly that this can used to fit the location of these "patch points" to a set of datapoints by minimusing a least-squares error 
%\end{itemize}

\subsection{Fitting Problem: Parametric Surfaces}
In order to fit a \Bez surface or NURBS surface to a set of datapoints with fixed locations and parameters on this surface, linear least-squares fitting can be applied, using an approach of mapping the parameters of the datapoints to control points on the surface\tododone{which approach? Erik:-The approach that was deleted after I wrote the section}. The base of the approach is using either Bernstein polynomials (\Bez curves) or B--spline basis functions (NURBS) to evaluate how control points should be combined for those surface parameters, and obtain this as a linear combination on the control points. That means, that from the parameters $(u,v)$ of each data point on the surface, we get a set of coefficients on the control points that define the surface. Expressing this as a sum, we have for the point $\lsDataPointi{k}$ with the parameters $(u_k,v_k)$ on the surface defined by the $N \times M$ control points $\lsControlPointi{i,j}$:
\begin{equation}
\lsDataPointi{k} = \sum\limits_{i,j=1}^{N,M} \lsControlPointCoefi{i,j}(u_k,v_k) \lsControlPointi{i,j}
\end{equation} 
where $\lsControlPointCoefi{i,j}(u,v)$ are the coefficients calculated on control point $\lsControlPointi{i,j}$ from the parameters $(u,v)$. Realizing that the matrix indexing $i,j$ can be flattened to a vector index $p = 1,2,...,S$, where $S=N\times M$, mapping to $i$ and $j$, we can express this as:
\begin{equation}
\lsDataPointi{k} = \sum\limits_{p=1}^{S} \lsControlPointCoefi{p}(u_k,v_k) \lsControlPointi{p} \equiv \vec{\lsControlPointCoef}(u_k,v_k)\lsControlPointMatrix
\label{eqn:lsmatrix1}
\end{equation} 
where in the last equality, we have expressed the control points as a vector-matrix product, where the $S\times 3$ matrix $P$ has the $p^\text{th}$ row as the position of the $p^\text{th}$ control points as a 3D row-vector, and $\vec{\lsControlPointCoef}(u_k,v_k)$ is the row vector whose entries are the coefficients $\lsControlPointCoefi{p}(u_k,v_k)$ on the control points. Now, again realizing that for $Q$ datapoints $\lsDataPointi{k}$ we can view them as the columns in a $Q\times 3$ matrix $\lsDataPointMatrix$, and create an analogous matrix $\lsControlPointCoefMatrix(\vec{u},\vec{v})$ with the $k^\text{th}$ containing the coefficients calculated from the parameters $(u_k,v_k)$ of datapoint $\lsDataPointi{k}$, resulting in the matrix equation:

\begin{equation}
\lsDataPointMatrix = \lsControlPointCoefMatrix(\vec{u},\vec{v})\lsControlPointMatrix
\label{eqn:lsmatrix2}
\end{equation}

Now, in the case that we have a set of datapoints that we manage to parametrize to the surface, we can calculate the positions of the control points by calculating the control point matrix $\lsControlPointCoefMatrix$ using these parameters, and then solving the system for $P$:
\begin{equation*}
\lsControlPointMatrix = \left[\lsControlPointCoefMatrix(\vec{u},\vec{v})\right]^{-1}\lsDataPointMatrix
\end{equation*}
However, this requires that there be a solution, which typically relies on the specific number of datapoints being exactly equal to the number of control points, and on the fact that here we also interpolate the points exactly. In most cases there are many more datapoints, and we try to find the $\lsControlPointMatrix$ that minimizes the least-squares error:
\begin{equation}
\norm{\lsDataPointMatrix - \lsControlPointCoefMatrix(\vec{u},\vec{v})\lsControlPointMatrix}^2
\label{eqn:lsminim}
\end{equation}
This again is a central problem in computing, and can be solved by many standard libraries with excellent performance.

\subsection{Fitting Problem: Fairness}
\label{subsec:lsqfairness}
As discussed in \autoref{subsec:fairnessthry}, it can also be important to have surface patches that are not only continous and smooth, but also have a low curvature. Fortunately, this is relatively simple to incorporate in the above Least-Squares formulation. Starting from the end result of \autoref{eq:fairnessnorm}, we see that the minimisation of the fairness functional can already be expressed as the minimisation of a squared norm of a matrix-vector product. In order to combine this with the fitting to a surface, we begin by writing it in the form of \autoref{eqn:lsminim}. To do that we simply replace $\lsDataPointMatrix$ with a 3-dimensional $0$-vector (thus a 3-column $0$-matrix), as we want to get the matrix-vector product as close to $0$ as possible, and minimize
\tododone[inline]{are the following two formulas really equal or is the minimization of both equivalent? The square for the second expression looks strange... Erik: Yes, strange indeed. woop, look, suddenly a magic power of two appeared!!! :)}
\begin{align*}
\phantom{=}& \norm{0 - \Lambda^{\frac{1}{2}} S \lsControlPointMatrix}^2 =
\\
=& \norm{0 - \lsControlPointCoefMatrix^{fair}\lsControlPointMatrix}^2,
\end{align*}
where $\lsControlPointCoefMatrix^{fair} = \Lambda^{\frac{1}{2}} S$ has the same number of columns as $\lsControlPointCoefMatrix(\vec{u},\vec{v})$ in \autoref{eqn:lsminim}. %The last fact allows us to minimize
To minimize both equations simultaneously, we notice that both terms are positive, and we can add them together, giving a weight $\lambda \geq 0$ to the fairness term:
\begin{equation}
\norm{\lsDataPointMatrix - \lsControlPointCoefMatrix(\vec{u},\vec{v}) \lsControlPointMatrix }^2 + \lambda \norm{0 - \lsControlPointCoefMatrix^{fair}\lsControlPointMatrix }^2
\end{equation}
Noticing that the terms in the sums each depend on one row of $D$ or $0$ and $\lsControlPointCoefMatrix(\vec{u},\vec{v})$ or $\lsControlPointCoefMatrix^{fair}\lsControlPointMatrix$ respectively, we see that we could also join the equations into one norm, by concatenating $D$ and $0$, and $\lsControlPointCoefMatrix(\vec{u},\vec{v})$ and $\lsControlPointCoefMatrix^{fair}\lsControlPointMatrix$ vertically, giving a minimization of
\begin{equation*}
\norm{
\begin{pmatrix}
\lsDataPointMatrix \\ 0
\end{pmatrix} -
\begin{pmatrix*}[l]
%for aligning the \lsControlPointCoefMatrix to the left. pmatrix* is from mathtools package
\phantom{\lambda}\lsControlPointCoefMatrix(\vec{u},\vec{v}) \\ \lambda \lsControlPointCoefMatrix^{fair}
\end{pmatrix*}
\lsControlPointMatrix 
}^2 =
\end{equation*}
\begin{equation}
\label{eq:lsconcatmat}
= \norm{\lsDataPointConcMatrix - \lsControlCoefConcMatrix(\vec{u},\vec{v}) \lsControlPointMatrix }^2
\end{equation}
\todointernal[author=Benni]{added a $^2$ in the second formula. That's right?}
where we have denoted the final concatenated matrices with $\lsDataPointConcMatrix$ and $ \lsControlCoefConcMatrix(\vec{u},\vec{v})$ respectively.

\subsection{Fitting Problem: Peters' Scheme}
\label{subsub:petersleastsq}
One of the major drawbacks with the approach above is that it is typically very complicated to impose smoothness constraints on structures of multiple patches, something that is essential for representing more complicated geometries. One of the ways of avoiding this is to use an existing scheme for creating a smooth surface, such as the scheme of Peters \cite{peters1992constructing,eck1996automatic}, which we introduced in \autoref{subsec:peters}. Here, we create smoothly connected square \Bez patches by letting their control points be linear combinations of vertices in a refined control mesh $\petersPatchPoints$. We ensure smoothness by the way we calculate these coefficients on the vertices in $\petersPatchPoints$ -- we use the information on their local positions in the mesh to determine which neighbouring vertices they should be influenced by. Once more, we can express this mathematically with $\lsControlPointi{p}$ denoting the $p^\text{th}$ control point, and $\petersPatchPointVec_l$ being the $l^\text{th}$ refined control mesh vertex:
\begin{equation}
\lsControlPointi{p} = \sum\limits_{l} \petersControlPointCoef_{p,l} \petersPatchPointVec_l
\end{equation} 
with $\petersControlPointCoef_{p,l}$ being the coefficient for the $p^\text{th}$ \Bez control point on the $l^\text{th}$ refined control mesh vertex as obtained from Peters' scheme. Analogous to equations \ref{eqn:lsmatrix1} and \ref{eqn:lsmatrix2}, we can extend this to all \Bez control points on all patches, and cast it in a matrix form to obtain:

\begin{equation}
\lsControlPointMatrix = \petersControlPointCoefMatrix\petersPatchPointMatrix
\end{equation}
where $\lsControlPointMatrix$ as before is the \Bez control point position matrix, $\petersControlPointCoefMatrix$ is the matrix of coefficients for the \Bez control points from on the refined control mesh vertices of $\petersPatchPoints$, and $\petersPatchPointMatrix$ is the matrix of the positions of these vertices. 

Finally, we can now substitute this into our equations \ref{eqn:lsmatrix2} and \ref{eqn:lsminim} above, to solve the system:

\begin{equation}
\lsDataPointConcMatrix = \lsControlCoefConcMatrix(\vec{u},\vec{v})\lsControlPointMatrix = \lsControlCoefConcMatrix(\vec{u},\vec{v})\petersControlPointCoefMatrix\petersPatchPointMatrix
\end{equation}
or minimize the least squares error for the only unknowns in this case, the positions of the refined control mesh vertices $\petersPatchPointMatrix$:
\begin{equation}
\norm{\lsDataPointConcMatrix - \left [\lsControlCoefConcMatrix(\vec{u},\vec{v})\petersControlPointCoefMatrix\right ]\petersPatchPointMatrix}^2
\label{eqn:petersminimisation}
\end{equation}

To summarize, using the standard methods of minimisation of least-squares errors, we compute a set of points, which when used in Peters' scheme produces a surface with the smallest error to a set of datapoints. By including a Fairness term, we also reduce the curvature of the surface.

For the computation of  $\lsControlPointCoefMatrix(\vec{u},\vec{v})$ (and therefore of $\lsControlCoefConcMatrix(\vec{u},\vec{v})$), we require the parametrisations of these datapoints on the surface, and in order to obtain $\petersControlPointCoefMatrix$, we also need to know how the patches connect with each other. 

It is worth noting that all matrices $\lsControlPointCoefMatrix(\vec{u},\vec{v})$, $\lsControlPointCoefMatrix^{fair}$ and $\petersControlPointCoefMatrix$ are, although likely very large for large amounts of datapoints and patches, also very sparse. This fact comes from that both $\lsControlPointCoefMatrix(\vec{u},\vec{v})$ and $\lsControlPointCoefMatrix^{fair}$ will only contain coefficients on one patch, and that the algorithms for computing $\petersControlPointCoefMatrix$ only use information of a few vertices around the patch. Thus, the final combined coefficient matrix $\lsControlCoefConcMatrix(\vec{u},\vec{v})\petersControlPointCoefMatrix$ will also have high sparsity. For solving the minimisation problem of \autoref{eqn:petersminimisation}, we will therefore be able to use specialised, fast solvers for sparse systems, with lower complexity, and therefore much lower runtime.
