\chapter{Background Theory}
\label{chapter:Background}

\section{CAD in computers}
%interaction with it, programs, geometry represenations, datastructures, formats etc., maybe even history if we're overkill
\subsection{History of CAD}
Computer aided design (short: CAD) refers to the process of designing a product using a computer system. Before CAD applications were used, products were constructed using a sketch board. It was a challenge to incorporate changes in the construction drafts as well as to keep documentations up to date; hence, it was no surprise that CAD systems spread rapidly across all design development branches. Computer aided design is now irreplaceable used in architecture, mechanical, electrical and civil engineering.

Depending on the discipline different requirements are set on the virtual model. One may imagine that in a civil engineering model of a building a 2D floor plan is often sufficient; however in the design of a mechanical motor a 3D model is always necessary. Given these circumstances, various CAD software bundles evolved in the different disciplines with completely different modelling approaches. Besides the geometry representation parameters, such as material properties or manufacturing information, are stored. In order to move between different data structures standardized exchange interfaces are commonly used.
\subsection{Geometry representations}
In general, two different ways of describing a geometry are used in CAD systems: constructive solid geometry consisting of a set of primitive forms or storing the boundary of a part assuming that the interior is filled (BREP). Other approaches, such as a complete voxelised geometry are not common due to extensive memory consumption.
\subsubsection{Constructive solid geometry}
 insert figure csg tree from wikimedia \\
One way of representing a geometry in CAD is the approach of \emph{constructive solid geometry} (short: CSG). The basic idea is to start from a set of primitives, e.g. a sphere, cylinder and cube. Basic Boolean operations link these primitives towards a complex geometry. 

Key advantages of this format is the precise representation using very few storage memory. However, not all desired forms can be represented by CSG and hence, a second type of geometry description is needed. 
\subsubsection{Boundary representation}
A different kind of modelling approach is the so-called \emph{boundary representation}. Instead of storing the geometry information at every single point, \emph{BREP} formats only save the boundary surface of the body. The interior is assumed to be uniformly filled. Especially in big geometries this approach simplifies the model immensely to an extend that amounts of data are better to handle. Surfaces can be stored as tetrahedrons (see later STL files) or in NURBS patches.
Furthermore, holes in the body are possible by saving the surface normal of the boundary surface. 

By the boundary representation arbitrary geometries can be created. Data amounts to fulfill a certain precision are larger than by the csg representation, but BREP files are usually easier to work with. Beware, that unreal geometries can be created using BREP formats.
\subsection{Data exchange file formats}


\section{Topology Optimisation}
%how the algorithms work, maybe what it can be used for

\section{From CAD to Voxels}


%BEGIN JC
\section{From Voxels to a surface representation}
\subsection{Isosurface Contouring}
Now when the optimized voxel data was obtained, the next step is to generate a \emph{mesh based
geometry}. It will be useful in the further NURBS implementation. In order to achieve it, the
data will be represented by a contour of a smooth function, rendering an isosurface. The
isosurface allows to visualize Scalar Volumetric Data in 3D. It furthermore permits a mesh
representation of the volume data. The mesh can be composed of triangles or quads, according
to the algorithm used. There are two main approaches to solve this problem, the most
famous one is Marching Cubes.

\begin{figure}
\centering
   \scalebox{0.8}{\includegraphics{Pictures/cubes.pdf}}\\
   \caption{Basis cases of Marching Cubes}
\end{figure}

\subsubsection{Marching Cubes} 

This algorithm takes as an input a regular volumetric data set and extracts a polygonal mesh. It
divides the space into cubes, which are defined by the volume information. Each cube has scalar
information on its vertices, the value is equal or above a marked isovalue. Therefore each of the
eight vertices of a cube can be marked or unmarked. According to these values vertices are drawn
on the edges of the cube at calculated points with the use of interpolation. A cube that contains
an edge is called active. Non active cubes are not further considered in the algorithm.

By connecting the vertices we obtain a polygon on each cube. There are 256 possible scenarios,
but most of them are just reflections or rotationally symmetric cases of each other. Therefore
there are 15 base cases which represent all the possibilities of the marching cubes (Figure 2.1). 
The original algorithm presents two main problems. Firstly it does not guarantee neither
correctness nor topological consistency, which means that holes may appear on the surface due
to inaccurate base case selection. The second problem is ambiguity, which appears when two
base cases are possible and the algorithm chooses the incorrect one. These cases can be grouped
into face ambiguities and internal ambiguities. There are many extended Marching Cubes
algorithms that tackle the problems of the original one, getting rid of the ambiguities and
providing correctness.

\subsubsection{Dual Contouring}
The idea of this algorithm is similar to Marching Cubes, but instead of generating vertices on the
edges of the cubes, it locates them inside the cube. Figure 2.5 shows the basic differences in both approaches.
The vertices associated with the four contigous cubes are joined and form a quad. The question now is
which place inside the cube is the ideal one to insert each vertex. Different dual algorithms are classified 
according to the answer for this question. Dual contouring generates a vertex positioned at the minimizer of a
quadratic function which depends on the intersection points and normals. Therefore the method needs Hermite 
data to work with.
\begin{equation*}
E(x)= x^TA^TAx-2x^TA^Tb+b^Tb
\end{equation*}
Where \textit{A} is a matrix whose rows are the normals and \textit{b} is a vector whose entries are the product of normals and intersection points. To solve this system, a nummerical treatement is needed. As proposed in [1] the best approach is to compute the
SVD decomposition of \textit{A} and form the pseudo-inverse by truncating its small singular values. 
%1=Dual Contouring of HErmite DAta


The main advantage of this method over MC is the adquisition of better aspect ratios. On the other hand the need of Hermite Data
represents a disadvantage. Furthermore there is no open source algorithm that implements the Dual Contouring scheme.


\subsection{The VTK Toolbox}
\subsubsection{Installing VTK}
VTK was installed using the Linux platform, for it to be successfully implemented a gcc compiler
must be already on the machine. VTK offers the possibility to use Python, TLC or C++ for
development. VTK toolbox is actually a C++ library, which is implemented in other languages. We
decided to continue the project with C++ since it gives the possibility to explore the original code.
A few dependency problems were encountered, nevertheless they were easy to track back. If any
problems were to be found at installation time, please refer to the VTK Wiki where the procedure
is explained step by step.

\begin{figure}
\centering
   \scalebox{0.35}{\includegraphics{Pictures/bunny_MC.pdf}}\\
   \caption{\textit{Right:} The famous Standford Bunny. \textit{Left:} Main difference between MC and Dual methods }
\end{figure}

\subsubsection{Implementing the VTK Classes}
The VTK toolbox was used in order to implement the algorithms on our optimized data. It is a heavily object
oriented toolbox. Our first approach was to use the built in Marching Cubes algorithm,
nevertheless it did not work with our unstructured grid data. It just works for ImageData and
PolyData . For structured and unstructured grids the tool to render the isosurface is the contour
Filter tool. Unfortunately the documentation does not present which algorithm the tool uses. It
can be inferred that it is an extended Marching cube algorithm.

The Contour Filtering seemed to work fine but the visualization of our data was still not possible
and an intermediate step was needed. We used the Implicit Modelling tool which is a filter that
computes the distance from the input geometry to the points of an output structured point set.
This distance function can then be "contoured" to generate new, offset surfaces from the original
geometry. It finally allowed visualization but it created one problem. Holes are lost in
the process.

\begin{figure}
\centering
   \scalebox{0.4}{\includegraphics{Pictures/contouring.pdf}}\\
   \caption{Contour Filtering tool after Implicit Modelling}
\end{figure}

A further idea to solve this problem is to convert at the first step the volume data into point data
and only then present it to the Contour Filtering Tool. This will be implemented in the next
milestone.

The next step was to create a coarser mesh from the fine one. The triangles that represent the
isosurface can be reduced with the Decimation tool. A smoothing step is necessary in between
to get the new connections right. The top part of figure [2] shows a 50 \% reduction of the
triangles, a noticeable difference can not be perceived. On the lower part a 90 \% reduction is
obtained, it is nevertheless still difficult to see a difference. Triangle meshes can be easily
coarsened since there are many open source algorithms that simplify the triangles. VTK has the
decimation tool which works for 3D triangle data.

\begin{figure}
\centering
   \scalebox{0.4}{\includegraphics{Pictures/Decimation.pdf}}\\
   \caption{Decimation of triangles. \textit{Top:} 50\% \textit{Lower:}90\%}
\end{figure}

\subsection{The Long Road to NURBS}
There are two possible roads to go from the voxel data to the NURBS representation.
\subsubsection{Quad Contouring}
This approach uses the dual contouring algorithm as first step in order to obtain a quad mesh
representation from the voxel data. The first challenge is to implement correctly the algorithm
with the ideas presented in [1]. The original marching cubes algorithm is
implemented in VTK but the source code is not public, therefore not only an extension
of it is needed, but a full implementation. Once this first step is done, the quads will be chosen for the
NURBS parametrization. A second step considers multiple smaller quads which have to be
combined into one larger patch. This is another challenge, since the remeshing of quad meshes
is not as straight forward as with the triangles. Different approaches have been taken in order to
achieve this coarsening. In [2] an incremental and greedy approach, which is based on local operations only, is presented. It depicts an iterative process which performs local optimizing, coarsening and smoothing operations. Another approaches, like
the one presented in [3] uses smooth harmonic scalar fields to simplify the mesh.
%2= “Practical quad mesh simplification”
%3= “Harmonic Functions for Quadrilateral Remeshing of Arbitrary Manifolds”


\subsubsection{Multiresolution Analysis of Arbitrary Meshes}
With this approach there is no need to apply a Dual Contouring algorithm, since it takes as
beginning data the triangles from the Marching Cubes. The main concepts are shown in the paper
of the same name [4]. It mainly takes a series of intermediate steps which permits a parametrization of data. It includes a partitioning scheme based on the ideas of the Voronoi Diagrams and Delaunay triangulations. Large patches or quads are obtained with this method. Further discussion on this approach is explained in the following section.

%4=reference to MAAM, a.k.a Benni's favorite paper!

Both approaches have not been implemented in open source documentation, therefore it will be
a long road to achieve what is required. For the first part the second
road was chosen and in case it leads to a dead end, the first way will be taken into consideration.

%END JC

\section{From a surface representation to NURBS}
\subsection{Bezier Curves}
\emph{Bezier curve} is a \textit{parametric} curve, which often used for producing a smooth approximation of a given set of data points.
\subsubsection{Analytical expression} 
An analytical expression for the Bezier curve is given by:
\begin{equation*}
\vec{B}(t)=\sum\limits_{i=0}^n b_i^n(t) \vec{P}_i,
\end{equation*}
where $\vec{P}_i$ is the $i$--th control point (we have $n+1$ control points). And
\begin{equation*}
b_i^n(t)=\left(\begin{array}{c}n\\i\end{array}\right)(1-t)^{(n-i)}t^i
\end{equation*}
is the $i$--th Bernstein polynomial of degree $n$.

Additionally to the expression with the Bernstein polynomials, one can use a recursion formula (\textbf{de Casteljau Algorithm}) for the construction of the Bezier--curve, which we will not cover here.
\subsubsection{Surfaces}
Analogically to Beziercurves, but with $n\cdot m$ Points $\vec{P}_{i,j}$ and the analytical expression
\begin{equation*}
\vec{S}(u,v)=\sum\limits_{i=0}^n \sum\limits_{j=0}^m b_i^n(u) b_j^m(v) \vec{P}_{i,j}
\end{equation*}
one can define a \textit{Bezier surface}
\\
\begin{mdframed}
Bezier--curves and surfaces may be unstable! Minor changes in control points might lead to major global changes!
\end{mdframed}


\subsection{NURBS basis functions}
Extending the idea, described in previous section, one could use NURBS basis functions instead of simple Bezier curves. 

Unlike Bezier curves, for the B-spline basis a parameter domain is subdivided with, so-called, \textit{knots}. In particular, given the parameter domain $[u_{0}, u_{m}]$ (in 1D), \textit{knot vector} is given by $u_{0} \leq u_{1} \leq ... \leq u_{m}$. In most cases $u_{0} = 0, u_{m} = 1$, so we get unit interval for our parameter values. Recall, that, N in NURBS stands for \textit{non-uniform}. This means, that our knots $u_{0},..., u_{m}$ are not equidistant.

Given \textit{knot vector} $[u_{0}, u_{m}]$ and a degree of B-spline $p$ one can find $i$-th B-spline basis function recursively as follows:
\begin{equation}
N_{i,0}(u) =  \begin{cases} 1, & \mbox{if } u_{i} \leq u < u_{i+1} \\ 0, & \mbox{otherwise } \end{cases}
\end{equation} 
\begin{equation}
N_{i,p}(u) = \frac{u - u_{i}}{u_{i+p} - u_{i}}N_{i, p-1}(u)  + \frac{u_{i+p+1}-u}{u_{i+p+1} - u_{i+1}}N_{i+1, p-1}(u)
\end{equation}
For $p=0$ we get just step functions (see fig. \ref{fig:bspline_basis_constant}), for $p=1$ we get familiar hat functions (see fig. \ref{fig:bspline_basis_linear}). Quadratic basis looks a bit more complicated, but also quite intuitive (fig. \ref{fig:lognorm_quadratic}).
\begin{figure}
\centering
\begin{subfigure}[B-spline basis for $p=0$]{
  \includegraphics[width=.4\linewidth]{basis_constant.eps}}
  \label{fig:bspline_basis_constant}
\end{subfigure}%
\begin{subfigure}[B-spline basis for $p=1$]{
  \includegraphics[width=.4\linewidth]{basis_linear.eps}}
  \label{fig:bspline_basis_linear}
\end{subfigure}
\begin{subfigure}[B-spline basis for $p=2$]{
  \includegraphics[width=.4\linewidth]{basis_quadratic.eps}}
  \label{fig:lognorm_quadratic}
\end{subfigure}
\caption{B-spline basis functions}
\end{figure}

Given these basis functions, Non-Uniform Rational B-Spline curve is given by:
\begin{equation}
C(u) = \frac{\sum_{i=1}^{k}N_{i,n}\omega_{i}P_{i}}{\sum_{i=1}^{k}N_{i,n}\omega_{i}},
\end{equation}
where $k$ is number of points, $\{P_{i}\}$ are given control points.

B--splines are have the following properties, which are useful for our problem:
\begin{itemize}
\item degree $n$ and number of control points $\vec{P}_{i\cdots m}$ are independent!
\item B--Splines only change locally (depends on the degree $n$) when a control point is changed.
\end{itemize}

Analogically, one can define B--spline surfaces.
\subsection{Minimization problem}
Now, once we defined all necessary tools, we proceed to the fitting problem.
\begin{mdframed}
\textbf{The goal:} Fit in a parametric curve to the set of given data points. 
\end{mdframed}
In our case our given set of points is a mesh, obtained on a previous step.
\subsection{Minimization Problem: Bezier curve}
First we want to find a Bezier--curve $\vec{B}_n\left(t\right)$ of degree $n$ which is approximating a given spline $\vec{s}_m\left(t\right)$ defined by $m$ points in a optimal way. For this purpose we want to minimize the L2--error. This leads to the minimization problem:
\begin{equation}
\text{find\ } \underset{\vec{B}_n\in \mathbb{B}_n}{\min} \Ltwonorm{\vec{B}_n-\vec{s}_m}
\end{equation}
The function space $\mathbb{B}_n$ is composed of functions of the following form:
\begin{equation}
\vec{B}_n\left(t\right)=\sum\limits_{i=0}^n a_{2i}\left(
\begin{array}{c}
b_i^n (t)\\0
\end{array}
\right)
+ a_{2i+1}
\left(
\begin{array}{c}
0\\b_i^n (t) 
\end{array}
\right)
\end{equation}
Therefore a control point for the Bezier--curve has the following form:
\begin{equation}
\vec{P}_i=\left(
\begin{array}{c}
a_{2i}\\a_{2i+1}
\end{array}
\right).
\end{equation}
Minimizing the L2--norm is equal to minimizing the functional:
\begin{equation}
F(\vec{B}_n)=\int\limits_{t=0}^{t=1} \left(\vec{B}_n-\vec{s}_m\right)^2\dt
\end{equation}
Varying $B_n$ yields
\begin{equation}
\delta F(B_n)=2\int\limits_{t=0}^{t=1} \left(\vec{B}_n-\vec{s}_m\right)^T \delta \vec{B}_n\dt
\end{equation}
using the definition of $\vec{B}_n$ and $\delta \vec{B}_n$
\begin{align}
\vec{B}_n\left(t\right) &=\sum\limits_{i=0}^n a_{2i}\left(
\begin{array}{c}
b_i^n (t)\\0
\end{array}
\right)
+ a_{2i+1}
\left(
\begin{array}{c}
0\\b_i^n (t) 
\end{array}
\right)
\\
\delta \vec{B}_n\left(t\right) &=\sum\limits_{i=0}^n \delta a_{2i}\left(
\begin{array}{c}
b_i^n (t)\\0
\end{array}
\right)
+ \delta a_{2i+1}
\left(
\begin{array}{c}
0\\b_i^n (t) 
\end{array}
\right)
\end{align}
leads to
\begin{multline}
\delta F(B_n)=2\int\limits_{t=0}^{t=1} \left(\sum\limits_{i=0}^n  a_{2i} \left(
\begin{array}{c}
b_i^n (t)\\0
\end{array}
\right)
+ a_{2i+1}
\left(
\begin{array}{c}
0\\b_i^n (t) 
\end{array}
\right)-\vec{s}_m\right)^T
\\
\left(\sum\limits_{j=0}^n \delta a_{2j} \left(
\begin{array}{c}
b_j^n (t)\\0
\end{array}
\right)
+ \delta a_{2j+1}
\left(
\begin{array}{c}
0\\ b_j^n (t) 
\end{array}
\right)\right)\dt \overset{!}{=}0
\end{multline}
rewritten
\begin{multline}
\int\limits_{t=0}^{t=1} \left(\sum\limits_{i=0}^n  a_{2i}
\left(
\begin{array}{c}
b_i^n (t)\\0
\end{array}
\right)
+  a_{2i+1}
\left(
\begin{array}{c}
0\\b_i^n (t)
\end{array}
\right)\right)^T
\left(\sum\limits_{j=0}^n 
\delta a_{2j} \left(
\begin{array}{c}
b_j^n (t)\\0
\end{array}
\right)
+ \delta a_{2j+1}
\left(
\begin{array}{c}
0\\ b_j^n (t) 
\end{array}
\right)\right)\dt 
\\
=
\int\limits_{t=0}^{t=1}\vec{s}_m^T \left(\sum\limits_{j=0}^n \delta a_{2j} \left(
\begin{array}{c}
b_j^n (t)\\0
\end{array}
\right)
+ \delta a_{2j+1}
\left(
\begin{array}{c}
0\\ b_j^n (t) 
\end{array}
\right)\right)\dt
\end{multline}
or using the notation
\begin{align}
\phi_{2i}&=
\left(
\begin{array}{c}
b_i^n (t)\\0
\end{array}
\right)
\\
\phi_{2i+1}&=
\left(
\begin{array}{c}
0\\b_i^n (t)
\end{array}
\right)
\end{align}
we get
\begin{equation}
\int\limits_{t=0}^{t=1} \left(\sum\limits_{i=0}^{2n+1} a_{i}\phi_i
\right)^T
\left(\sum\limits_{j=0}^{2n+1}
\delta a_{j} \phi_j\right)\dt 
=
\int\limits_{t=0}^{t=1}\vec{s}_m^T \left(\sum\limits_{j=0}^{2n+1} \delta a_{j} \phi_j\right)\dt
\end{equation}
or
\begin{equation}
\sum\limits_{i=0}^{2n+1} \sum\limits_{j=0}^{2n+1}
a_{i} \delta a_{j}\int\limits_{t=0}^{t=1} \phi_i^T
\phi_j\dt 
=
\sum\limits_{j=0}^{2n+1} \delta a_{j} \int\limits_{t=0}^{t=1}\vec{s}_m^T \phi_j\dt
\end{equation}
this has to be true for any $j$ and $\delta a_j$, therefore 
\begin{equation}
\sum\limits_{i=0}^{2n+1} a_{i} \int\limits_{t=0}^{t=1} \phi_i^T
\phi_j\dt 
=
\int\limits_{t=0}^{t=1}\vec{s}_m^T \phi_j\dt \ \forall j=0\cdots 2n+1
\end{equation}
using
\begin{equation}
A_{ij}=\int\limits_{t=0}^{t=1} \phi_i^T
\phi_j\dt 
\text{\ and \ }
b_j=\int\limits_{t=0}^{t=1}\vec{s}_m^T \phi_j\dt
\end{equation}
we get the linear equation system
\begin{equation}
A a = b
\end{equation}
which has to be solved for $a$.

\subsection{Minimization problem(least squares): NURBS}
Although the approach used above showed good results, it appears to be very computationally extensive. Analogically, one could reduce the original problem to the \textit{linear regression problem}, which allows us to reduce computational costs. Also, to improve the locality of our solution, from now on we are going to use NURBS basis functions with weights $\omega_{j} = 1$ instead of Bezier curves.

Let $X^{0}$ be the $n \times 2$ matrix of the given set of points, $N^{p}$ - the basis functions of degree $p$ ($n \times (n+p)$ matrix, where $n$ - number of points), $P^{0}$ - the control points ($(n+p) \times 2$ matrix).

The original problem can be written as:
\begin{equation}
X_{i}^{0} = \sum\limits_{j=1}^{n+p} P_{j}^{0} N_{i,j}^{p}, \quad i \in \{1,..,n\}
\end{equation}
Or, in short:
\begin{equation}
X^{0} = N^{p} P^{0}
\end{equation}
The above system needs to be solved for the unknown $P^{0}$. One of the ways to solve it is to use SVD decomposition. So far, we used built-in MATLAB solver.
\subsection{Reparametrization}
In our simple case we use coordinates of our points as a parameters, but even more advanced parametrization algorithms usually don't give an optimal parametrization. For handling this issue in (reference!!!) the following iterative algorithm was proposed:
\begin{enumerate}
\item \textit{Fitting step:} For \textbf{fixed parametrizations} $\{t_{i}\}$ , the optimal control points are found by solving a linear least-squares problem.
\item \textit{Parameter correction step:} For \textbf{fixed control points}, optimal
parametrizations $\{t_{i}\}$ are found by projecting the points onto our surface.
\end{enumerate}
The first part of the algorithm is exactly the problem we described in a previous section. The second part can be tackled by solving linearised system of equations, which provide us relatively cheap (we need to solve only a system of linear equations) and good (see fig. ) method for the parameter correction.
%\begin{figure}
%  \includegraphics[width=.85\linewidth]{parameterOpti.pdf}
%  \caption{Reparametrization}
%  \label{fig:reparametrization}
%\end{figure}
\subsection{Fitting pipeline}
\begin{figure}
  \includegraphics[width=.85\linewidth]{Fitting_workflow.png}
  \caption{Fitting pipeline}
  \label{fig:fitting_pipeline}
\end{figure}
Since the geometry obtained after topology optimization can be arbitrary complex, we might not be able to find a food fit using only one patch. We seek a multi step algorithm, allowing us to break the overall big problem into smaller problems, which can be handled relatively easy.
Based on the algorithm described in (reference to ...), our overall fitting pipeline looks as follows (see fig. \ref{fig:fitting_pipeline}):
\begin{itemize}
	\item Patch selection (breaking our problem in small pieces which can be solved using least squares)
	\item Parametrization of obtained patches
	\item B--spline fitting using least squares
	\item Smooth connection of patches
	\item Conversion back to CAD
\end{itemize}


